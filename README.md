# PerformAI

**PerformAI** is a modular Python tool that connects to a Kubernetes cluster, analyzes CPU and memory usage of workloads using Prometheus metrics, and generates optimization recommendations using a local LLM (like Mistral 7B via Ollama) or hosted LLMs compatible with OpenAI API.

---

## üöÄ Features
- Targets only specific namespaces
- Pulls resource usage from Prometheus over configured lookback duration
- Compares with actual CPU/Memory requests & limits
- Uses local or hosted LLM compatible with OpenAI API to get recommended adjustments

---

## üì¶ Requirements
- Python 3.9+
- Prometheus endpoint accessible
- `kubectl` configured and authenticated
- [Ollama](https://ollama.com/) running locally with the `mistral` model:
  ```bash
  ollama run mistral
  ```
- Optionally, hosted LLM URL, Model name, API Token

---

## üîß Setup
```bash
git clone https://github.com/yourusername/performAI.git
cd performAI
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## üõ† Configuration
Set the environment variables 
```commandline
export NAMESPACES="namespace1,namespace2,another-namespace"
export PROMETHEUS_URL="https://your-prometheus.example.com"
export LOOKBACK_DURATION="7d"
export CHUNK_SIZE="10"
export DEBUG="True"
```

---

## üß† Usage

For local LLM: 
```bash
python main.py
```
For hosted LLM:
```bash
USE_LOCAL_LLM=false \
LLM_HOSTED_URL=https://your-llm-host.com \
LLM_API_TOKEN=your-token-here \
LLM_MODEL=model-name \
python main.py
```
For web UI
```bash
python app.py
```
served on localhost:5000

![UI screen](./img/performAI.png)
![UI processing](./img/processing.png)
![UI results](./img/Results.png)

---

## üåê Available Environment Variables

| Variable           | Description                                              | Default          |
|--------------------|----------------------------------------------------------|------------------|
| `NAMESPACES`       | Comma-separated list of Kubernetes namespaces to monitor.| `threescale`     |
| `PROMETHEUS_URL`   | URL of the Prometheus instance to query.                 | ``               |
| `LOOKBACK_DURATION`| The time window to query Prometheus for metrics.         | `30d`            |
| `CHUNK_SIZE`       | The size of workload chunks sent to the LLM for analysis.| `5`              |
| `DEBUG`            | Enable debug logging (set to "true" for verbose output). | `false`          |
| `USE_LOCAL_LLM`    | Whether to use local Ollama (`true` or `false`)          | `true`           |
| `LLM_HOSTED_URL`   | Base URL of the hosted LLM endpoint                      | *(required if hosted)* |
| `LLM_API_TOKEN`    | API token to authenticate with hosted LLM                | *(required if hosted)* |
| `LLM_MODEL`        | Model name to use (e.g., `mistral`, `gpt-3.5-turbo`)     | `mistral`        |

These environment variables control which LLM is used and how it's accessed. When using a hosted LLM, ensure the URL and token are valid for the OpenAI-compatible `/v1/chat/completions` endpoint.

---

## üìÇ Project Structure
```
performai/
‚îú‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ collect.py
‚îú‚îÄ‚îÄ prompts.py
‚îú‚îÄ‚îÄ llm.py
‚îú‚îÄ‚îÄ utils.py  
templates/
‚îú‚îÄ‚îÄ index.html
‚îú‚îÄ‚îÄ processing.html
‚îú‚îÄ‚îÄ results.html     
app.py
main.py
```

---

## üìú License
MIT

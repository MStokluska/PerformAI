# PerformAI [WIP]

**PerformAI** is a modular Python tool that connects to a Kubernetes cluster, analyzes CPU and memory usage of workloads using Prometheus metrics, and generates optimization recommendations using a local LLM (like Mistral 7B via Ollama).

---

## ðŸš€ Features
- Targets only specific namespaces
- Pulls resource usage from Prometheus over configured lookback duration
- Compares with actual CPU/Memory requests & limits
- Uses local LLM (Mistral 7B) to recommend adjustments

---

## ðŸ“¦ Requirements
- Python 3.9+
- Prometheus endpoint accessible
- `kubectl` configured and authenticated
- [Ollama](https://ollama.com/) running locally with the `mistral` model:
  ```bash
  ollama run mistral
  ```

---

## ðŸ”§ Setup
```bash
git clone https://github.com/yourusername/performAI.git
cd performAI
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## ðŸ›  Configuration
Edit `performai/config.py` to set:
- Target namespaces
- Prometheus endpoint
- Lookback window for metrics

---

## ðŸ§  Usage
```bash
python main.py
```
---

## ðŸ“‚ Project Structure
```
performai/
â”œâ”€â”€ config.py
â”œâ”€â”€ collect.py
â”œâ”€â”€ prompts.py
â”œâ”€â”€ llm.py
â”œâ”€â”€ utils.py          
main.py
```

---

## ðŸ“œ License
MIT
